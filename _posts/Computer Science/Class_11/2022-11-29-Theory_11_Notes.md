---
title: Computer Science (Theory) for Class 11 (NEB)
date: 2022-11-29 15:00:25 + 0545
author: ajay
categories: [Computer Science, Class 11]
tags: [notes, class 11, computer science, thoery]     # TAG names should always be lowercase
math: true
---
## **1. Computer System**

### **1.1 Introduction of Computer**

A computer is an electronic device, operating under the control of instructions stored in its own memory that can accept data (input), process the data according to specified rules, produce information (output), and store the information for future use.  
The physical parts that make up a computer (the central processing unit, input, output, and memory) are called hardware. Programs that tell a computer what to do are called software. A set of instructions that perform a particular task is called a program, software program, or software. Peripherals are any hardware device connected to a computer, any part of the computer outside the CPU and working memory. Some examples of peripherals are keyboards, the mouse, monitors, printers, scanners, disk and tape drives, microphones, speakers, joysticks, plotters, and cameras. Computer is an advanced electronic device that takes raw data as input from the user and processes these data under the control of set of instructions (called program) and gives the result (output) and saves output for the future use. It can process both numerical and non-numerical (arithmetic and logical) calculations.

### **1.1.1 Definition, Characteristics and Application of Computer**

### Definition of Computer

A computer can be more accurately defined as an electronic device that takes data as input, stores and processes it and displays the output according to either the given instructions or the instructions stored in their memory unit.

### Characteristics of Computer

>The major characteristics of a computer can be classified into speed, accuracy, diligence, versatility, and memory which are as follows:
>
>* Speed: The computer can process the data and give the output in fractions of seconds such that required information is given to the user on time enabling the user to take the right decisions at the right time. A powerful computer can execute about 3 million calculations per second.
>* Accuracy: Inspire of its high speed of processing, the computers accuracy is consistently high enough which avoids any errors. If there are any errors, they are due to errors in instructions given by the programmer or input data.
>* Versatility: The computer is versatile in nature in the sense of working and purpose. What that means is we can use the computer in any way we want. For example, if we need some graphics work, we can install the graphics software and we can design and accomplish our graphics work at the same time. Later if we need some report writing we can certainly install the word processing software and accomplish that also on the same computer we have used as a graphics designer. Similarly, we can do any work if the application for the same is available to us. That’s why the computer is said to be versatile.
>* Automation: Once the instructions are fed into the computer it works automatically without any human intervention until the completion of execution of program until meets logical instructions to terminate the job.
>* Reliability: The output generated by the computer is very reliable, but it is reliable only when the data which is passing as input to the computer and the program, which gives instructions are correct and reliable.
>* Diligence: A computer can perform millions of tasks or calculations with the same consistency and accuracy. It doesn’t feel any fatigue or lack of concentration. Its memory also makes it superior to that of human beings.
>* Storage: The computer has a provision to store large volumes of data in the small storage devices, which have the capacity to store huge amounts of data and help the retrieval of data an easy task.
{: .prompt-tip }

### **1.1.2 Evolution of Computer Technology**

The computer is well-known to us, the first counting device our ancestors used in the early decades. But also, before that, they utilized sticks, bones, and stones as their counting implements. As the evolution of the human mind and technology improved with time, the existence of more computing devices increased. In this section, we will get to know details on the History of computers and the Timeline according to the subsequent years.

### First Computer Design

In the 19th century, a renowned Mathematician Charles Babbage developed and partly built a Victorian-era computer named the Analytical Engine. The fundamental component of the oldest machine was the input, having the programs and data that the user had to provide to the Analytical Machine through punched cards, a technique being employed at the time to handle mechanical looms like the Jacquard loom. The creation of the design of the Analytical Engine; was initiated by the year 1833.
The computer was born not for amusement or email but out of a requirement to solve a complicated number-crunching crisis. By 1880, the U.S. population had expanded so vastly; that it bore more than seven years to tabulate the U.S. Census consequences. The government pursued a quicker method to acquire the job done, offering an upgrade to the punch-card-based computers that grabbed up total room space.
From the 19th century to the present day, the role of the computer in its users' life is crucial. However, in today's generation, this computer may work a little differently and more advanced than in the 19th century. But it served the purpose it is to its users and remained the same. This tutorial describes various generations of computers in detail.

### List of Five Generations of Computers

The journey of five generations of computers begins with vacuum tube circuitry from the 1940s and goes beyond the methods and approaches of artificial intelligence (AI) to the present day. These are as follows:

### First Generation of Computers

By the year 1940, Vacuum tubes, an electronic device that regulates the flow of electrons in a vacuum, were used. These were the first computer systems that the users utilized for circuitry and magnetic drums and were usually massive, capturing up an entire room. These computers were very costly to operate in the spare of employing a great deal of electricity. At that time, the most common computer language that the first-generation computers depended on was the machine language, the lowest-level programming language that the computers understood for executing operations. The UNIVAC and ENIAC computers are specimens of the first-generation computing devices.  

> _**Characteristics of First Generation of Computers**_  
>
>* The main electronic component of first-generation computers is the vacuum tubes.
>* It operated in machine language.
>* Its primary memories were the Magnetic tapes and magnetic drums.
>* It employed its Input/output devices as Paper tape and punched cards.
{: .prompt-tip }

### Second Generation of Computers  

In 1956, the technology of transistors replaced the bulkier generation of vacuum tubes. After the invention of these transistors, the dimensions of the computer also reduced. Second-generation computers evolved smaller in size compared to first-generation computers. Second-generation computers developed from enigmatic binary machine language to representational symbolic systems, or assembly languages, that authorized the programmers to appoint instructions in words or phrases. IBM1400 series, PDP-8, IBM 7090 and 7094, UNIVAC 1107, CDC 3600, etc., are a few examples of the Second-generation.  

>_**Characteristics of Second Generation of Computers**_  
>
>* The main electronic component of second-generation computers is electronic transistors.
>* It operated in Machine language and assembly language.
>* Its primary memories were the Magnetic core and magnetic tape or magnetic disk.
>* Its Input/output devices were the Magnetic tape and punched cards.
{: .prompt-tip }

### Third Generation of Computers

This generation started developing integrated circuits in 1964. Instead of using punch cards and printouts, users were able to interact with third-generation computers via keyboards and monitors and interfaced with an operating system. For the first time, computers reached a mass audience, as they were smaller and cheaper than the past prototypes. Jack Kilby of Texas Instruments and Robert Noyce of Fairchild Semiconductor developed integrated circuits by 1950.

>_**Characteristics of Third Generation Computers**_  
>
>* The main electronic component of third-generation computers is integrated circuits.
>* It operated in High-level language.
>* Its primary memories were the large magnetic core and magnetic tape/disk.
>* Its Input/output devices were the Magnetic tape, monitor, keyboard, printer, etc.
{: .prompt-tip }

### Fourth Generation of Computers

By 1971, users operated the first microprocessors, the Large-Scale Integration (LSI) circuits created on one chip called microprocessors. The microprocessor was conducted in the fourth generation of computers, as developers built thousands of integrated circuits onto a single silicon chip. What if the first generation served an entire room that could currently accommodate within a palm? The Intel 4004 chip, developed in 1971, located all the computer components from the Central Processing Unit and memory to input or output authorities on a single chip.

>_**Characteristics of Fourth Generation of Computers**_
>
>* The main electronic component of fourth-generation computers is Very Large-Scale Integration (VLSI) and the microprocessor (VLSI contains thousands of transistors inside a single microchip).
>* It operated in High-level language.
>* Its primary memories were the semiconductor memory (mainly RAM, ROM, etc.)
>* Its Input/output devices were the pointing devices, optical scanning, keyboard, monitor, printer, etc.
{: .prompt-tip }

### Fifth Generation of Computers

The technology on which the fifth generation of computers relies is AI. It authorizes computers to conduct like humans. Today's computers are so developed; that the users utilize them in every distinct field, primarily accounting, constructing buildings, space research, engineering technologies, and other types of analysis. The principal purpose of fifth-generation computing is to create devices that react to natural language input, competent in learning and self-organizing.

>_**Characteristics of Fifth Generation of Computers**_
>
>* The main electronic component of fourth-generation computers is Ultra Large-Scale Integration (ULSI) and the parallel processing technique.
>* It operated in natural human language.
>* Its Input/output devices were the Trackpad, touch screen, pen, speech input, light scanner, etc.
{: .prompt-tip }

### **1.1.3 Measurement unit of processing speed and storage unit**

The central processing unit (CPU) of a computer is the part of the machine that retrieves and executes instructions. An arithmetic and logic unit (ALU), a control unit, and multiple registers make up the system. The processor is a common term for the central processing unit (CPU). These are also called the unit of electromagnetic (EM) wave frequency.

>The different measurement units and their relationship are as follows:  
1,000 Hz = 1 Kilo Hertz (103 Hz)  
1,000 KHz = 1 Mega Hertz (106 Hz)  
1,000 MHz= 1 Giga Hertz (109 Hz)  
1,000 GHz= 1 Tera Hertz (1012 Hz)
{: .prompt-tip }

The size of the device in computers does not reflect the space available to store data in it. There are larger devices that can store only a few data were as many tiny devices as possible that store an unbelievable amount of data. It is also one type of measurement unit.
Hence, we need to find some other way to measure space. All the digital computers use binary numbering systems (though there are some exceptions).
The binary numbering system consists of only two digits – 0 and 1 to represent any quantity. 10 in binary is equal to the 2 and 100 to 5. Everything in computers is represented in strings of binary numbers. For example, capital A is interpreted by the computer as 0100 0001 and B is 0100 0010. All characters, numbers, symbols, images, sounds, animations, videos, and everything is converted into suitable binary code to store on a computer or processed by computer.
So, if there is any device that can store one binary digit (whether 0 or 1), its storage capacity is 1 bit. Here, we have larger units that represent a group of lower units. A group of 4 binary digits is called a nibble (4 bits = 1 Nibble). Similarly, a group of 8 bits is called a byte (1 byte = 8 bits).
As you have seen in the example above, each character requires 8 bits which are 1 byte. So, 1 character requires 1-byte space. Now, if you have a text file whose size is 32 bytes, it means there are 32 x 8 binary digits (0s and 1s) stored in it.
Following table lists the different units and their values:

>| Storage Measurement Units | Units Equivalent |
|-----------------|------------------|
| 0 or 1 | 1 Bit |
| 4 Bits | 1 Nibble |
| 8 Bits | 1 Byte |
| 1024 Bytes | 1 Kilobyte (KB) |
| 1024 Kilobytes | 1 Megabyte (MB) |
| 1024 Megabytes | 1 Gigabyte (GB) |
| 1024 Gigabytes | 1 Terabyte (TB) |
| 1024 Terabytes | 1 Petabyte (PB) |
| 1024 Petabytes | 1 Exabyte (EB) |
{: .prompt-tip }

### **1.1.4 Super, Mainframe, Mini and Microcomputers**

>**Super Computer**  
They are the most expensive of all the computers. These computers are big general-purpose computers capable of executing more than 10,000 million instructions per second and have storage capacities of millions of bits per chip. These computers are used to solve multi- variate mathematical problems such as atomic nuclear and plasma physics seismology, aerodynamics etc.
Supercomputers are typically capable of handling hundreds of millions of floating points operations per second (MFLOPS). The speed of super computers generally measured in “FLOPS” (Floating Point Operations Per Second).
Super computers are used for highly calculation- intensive tasks such as weather forecasting, climate research, molecular modeling, physical simulation, and cryptanalysis and military and Scientific agencies are heavy users.
Some super computers are – Cray 1, Cray 2, Cray 3 perform 10 billion operation per second, Param, Cyber 810&830 etc.
{: .prompt-tip }
___
>**Mainframe Computer**  
They are very big in size and offer the maximum computing power. Many peripherals can be attached to them. They are generally used in large networks of computers with the mainframe being the model point of the network. They used satellites for networking. A typical application is the airline system. It has a mainframe computer at their head office where information on all the fights is stored. Small computers are installed at the booking offices are attached to central data bank, so that up-to-date information of all flights is always available.
Some computers are – Univac 1100/10, Univac 1100/60, Honeywell DSP 88/860, IBM 270/168 etc.
{: .prompt-warning }
___
>**Minicomputer**  
They are smaller versions of the mainframes. Generally, they offer the same computing power as bigger counterparts. The most important advantage of a minicomputer over the main frame is that it is cheaper in cost, smaller in size and reliable. It does not require air conditioning and can be operated in room temperature.
Main used of these systems is in education in local government word processing etc. in business they are being used for involving stock payroll etc. it is generally used as server system on networks with personal computers as nodes.
Some typical machines– TDC 316, PDP 11/70, Honeywell (XPS-100), HCL-4.
{: .prompt-info }
___
>**Micro Computer**  
>A microcomputer is a computer whose CPU is a microprocessor. A microprocessor is a processor all whose components are on a single integrated circuit chip.
>Personal computers are a kind of kind of microcomputer. Personal computers are called so because they are designed for personal use of individual or individual small business units’ office automation unit or professionals. Pc can be used for variety of applications like computer literacy, fun and games, business applications, programming etc.
>Types of Micro Computer or personal computers
>
>* Desktop Computer
>* Laptop Computer
>* Palmtop Computer, Digital Diary, Notebook, PDAs.
{: .prompt-danger }

### **1.1.5 Mobile Computing and it's Application**

Mobile computing is human–computer interaction in which a computer is expected to be transported during normal usage, which allows for the transmission of data, voice, and video. Mobile computing involves mobile communication, mobile hardware, and mobile software. Communication issues include ad hoc networks and infrastructure networks as well as communication properties, protocols, data formats, and concrete technologies. Hardware includes mobile devices or device components. Mobile software deals with the characteristics and requirements of mobile applications: -  

>Main Principles –  
>
>* Portability: Devices/nodes connected within the mobile computing system should facilitate mobility. These devices may have limited device capabilities and limited power supply but should have a sufficient processing capability and physical portability to operate in a movable environment.
>* Connectivity: This defines the quality of service (QoS) of the network connectivity. In a mobile computing system, the network availability is expected to be maintained at a high level with a minimal amount of lag/downtime without being affected by the mobility of the connected nodes.
>* Interactivity: The nodes belonging to a mobile computing system relate to one another to communicate and collaborate through active transactions of data.
>* Individuality: A portable device or a mobile node connected to a mobile network often denotes an individual; a mobile computing system should be able to adopt the technology to cater to the individual needs and to obtain contextual information of each node.
{: .prompt-tip }

### **1.2 Computer System and IO Devices**

### **1.2.1 Concept of Computer Architecture and Orgamization**

### **1.2.2 Components of Computer System: Input unit, Output unit, Processing unit, Memory unit and Storage**

### **1.2.3 Microprocessor: basic concepts, clock speed, word length, components and functions**

### **1.2.4 Bus System: data bus, address bus and control bus**

### **1.2.5 Primary Memory: Definition, RAM, ROM, Cache, Buffer, types of RAM and ROM**

### **1.2.6 Microprocessor: basic concepts, clock speed, word length, components and functions**

### **1.2.7 Input Devices: Keyboard, Mouse, Scanner, Light Pen, OMR, OCR, BCR, MICR Scanner, Touch Screen, Microphone and Digital Camera**

### **1.2.8 Output Devices: Monitor (LCD, LED), Printer (Dot Matrix, Inkjet, Laser), Speaker**

### **1.2.9 Hardware Interfaces: Parallel Port, Serial Port, USB Ports, HDMI and Expansion Slots**

## **2. Number System and Conservation Boolean Logic**

### **2.1 Number System and Conversion**

A numeral system (or system of numeration) is a writing system for expressing numbers; that is, a mathematical notation for representing numbers of a given set, using digits or other symbols in a consistent manner.

The same sequence of symbols may represent different numbers in different numeral systems. For example, "11" represents the number eleven in the decimal numeral system (used in common life), the number three in the binary numeral system (used in computers), and the number two in the unary numeral system (e.g. used in tallying scores).

The number the numeral represents is called its value. Not all number systems can represent all numbers that are considered in the modern days; for example, Roman numerals have no zero.

Ideally, a numeral system will:

* Represent a useful set of numbers (e.g. all integers, or rational numbers)
* Give every number represented a unique representation (or at least a standard representation)
* Reflect the algebraic and arithmetic structure of the numbers.

For example, the usual decimal representation gives every nonzero natural number a unique representation as a finite sequence of digits, beginning with a non-zero digit.

Numeral systems are sometimes called number systems, but that name is ambiguous, as it could refer to different systems of numbers, such as the system of real numbers, the system of complex numbers, the system of p-adic numbers, etc. Such systems are, however, not the topic of this article.

### **2.1.1 Decimal, Binary, Octal, Hexadecimal Number system and Conversion**

### **Binary Number System**

A binary number is a number expressed in the base-2 numeral system or binary numeral system, a method of mathematical expression which uses only two symbols: typically "0" (zero) and "1" (one).

The base-2 numeral system is a positional notation with a radix of 2. Each digit is referred to as a bit, or binary digit. Because of its straightforward implementation in digital electronic circuitry using logic gates, the binary system is used by almost all modern computers and computer-based devices, as a preferred system of use, over various other human techniques of communication, because of the simplicity of the language and the noise immunity in physical implementation.

### **Counting in binary**

Counting in binary is similar to counting in any other number system. Beginning with a single digit, counting proceeds through each symbol, in increasing order. Before examining binary counting, it is useful to briefly discuss the more familiar decimal counting system as a frame of reference.

### **Decimal counting**

Decimal counting uses the ten symbols 0 through 9. Counting begins with the incremental substitution of the least significant digit (rightmost digit) which is often called the first digit. When the available symbols for this position are exhausted, the least significant digit is reset to 0, and the next digit of higher significance (one position to the left) is incremented (overflow), and incremental substitution of the low-order digit resumes. This method of reset and overflow is repeated for each digit of significance. Counting progresses as follows:

  >000, 001, 002, ... 007, 008, 009, (rightmost digit is reset to zero, and the digit to its left is incremented)  
  010, 011, 012, ...  
     ...  
  090, 091, 092, ... 097, 098, 099, (rightmost two digits are reset to zeroes, and next digit is incremented)  
  100, 101, 102, ...
  {: .prompt-tip }

### **Binary counting**

Binary counting follows the exact same procedure, and again the incremental substitution begins with the least significant digit, or bit (the rightmost one, also called the first bit), except that only the two symbols 0 and 1 are available. Thus, after a bit reaches 1 in binary, an increment resets it to 0 but also causes an increment of the next bit to the left:

>0000, 0001, (rightmost bit starts over, and next digit is incremented)  
0010, 0011, (rightmost two bits start over, and next bit is incremented)  
0100, 0101, 0110, 0111, (rightmost three bits start over, and the next bit is incremented)  
1000, 1001, 1010, 1011, 1100, 1101, 1110, 1111 ...
{: .prompt-tip }

### **Binary Number Conversion**

### **Binary to Decimal**

In the binary system, each bit represents an increasing power of $$2$$, with the rightmost bit representing $$ 2^0 $$, the next representing $$ 2^1 $$, then $$ 2^2 $$, and so on. The value of a binary number is the sum of the powers of $$2$$ represented by each "1" bit. For example, the binary number 100101 is converted to decimal form as follows:

>$$(100101)_{2}$$ = $$(1 * 2^5 + 0 * 2^4 + 0 * 2^3 + 1 * 2^2 + 0 * 2^1 + 1 * 2^{0})_{10}$$  
>$$(100101)_{2}$$ = $$(1 * 32 + 0 * 16 + 0 * 8 + 1 * 4 + 0 * 2 + 1 * 1)_{10}$$  
>$$(100101)_{2}$$ = $$(3710)_{10}$$
{: .prompt-tip }

| Decimal number | Binary number |
|-------------|---------------|
| 0  | 0 |
| 1 | 1  |
| 2 | 10  |
| 3 | 11  |
| 4 | 100  |
| 5 | 101  |
| 6 | 110  |
| 7 | 111  |
| 8 | 1000  |
| 9 | 1001  |
| 10 | 1010  |
| 11 | 1011  |
| 12 | 1100  |
| 13 | 1101  |
| 14 | 1110  |
| 15 | 1111  |

### **Binary to Decimal of Fraction**

So far we have only looked at whole numbers (integers), we need to understand how computers represent fractions.

You should have learned at Primary School how a decimal fraction works:

| $10^{1}$ | $10^{0}$ |  | $10^{-1}$ | $10^{-2}$ |
|------|:-----:|:-----:|:-----:|------:|
|10 | 1 |  | ${\frac {1}{10}}$ | ${\frac {1}{100}}$ |
|1 | 2 | . | 7 | 5 |

As you can see, the column headings have been extended to ${\displaystyle 10^{-1}={\frac {1}{10}}}$ and ${\displaystyle 10^{-2}={\frac {1}{100}}}$. We can do the same thing in binary with the column headings ${\displaystyle 2^{-1}={\frac {1}{2}}}$, ${\displaystyle 2^{-2}={\frac {1}{4}}}$, and so on. The number 12.75 in 8 bit binary with 4 bits after the binary point is therefore $8 + 4 + 0.5 + 0.25$:

### **Decimal to Binary Conversion**

An easy method of converting decimal to binary number equivalents is to write down the decimal number and to continually divide-by-2 (two) to give a result and a remainder of either a “1” or a “0” until the final result equals zero.

### Steps to convert the decimal to binary equivalent

* Divide the decimal number by $2$ and store remainders in array.
* Divide the quotient by $2$.
* Repeat step 2 until we get the quotient equal to zero.
* Equivalent binary number would be reverse of all remainders of step 1.

>So for example.  Convert the decimal number $294_{10}$ into its binary number equivalent.  
>Number is $294$  
>divide by 2  
>result 147 remainder **0**  (LSB)  
>divide by 2  
>result 73 remainder **1**  
>divide by 2  
>result 36 remainder **1**  
>divide by 2  
>result 18 remainder **0**  
>divide by 2  
>result 9 remainder **0**  
>divide by 2  
>result 4 remainder **1**  
>divide by 2  
>result 2 remainder **0**  
>divide by 2  
>result 1 remainder **0**  
>divide by 2  
>result 0 remainder **1**  (MSB)
{: .prompt-info }

This divide-by-2 decimal to binary conversion technique gives the decimal number $294_{10}$ an equivalent of $100100110_2$ in binary, reading from right to left. This divide-by-2 method will also work for conversion to other number bases.

### **Decimal to binary of Fraction**

You can convert the decimal number in to binary with the similar process done before but with an extra work for the fractional part of the given number. Repeat the process done before for the integer part of the given number and follow these extra step to convert the fraction part:

* Multiply the fractional decimal number by $$ 2 $$.
* Integral part of resultant decimal number will be first digit of fraction binary number.
* Repeat step 1 using only fractional part of decimal number and then step 2.
* And finally combine both integral and fractional part of binary number.

>Let's take an example for n = $$ 4.47 $$  
Step 1: Conversion of 4 to binary
{: .prompt-info }

| Calculations | Remainder | Quotient |
|----------|--------|----------|
| $4/2$ | Remainder = $$ 0 $$ | Quotient = $$ 2 $$ |
| $2/2$ | Remainder = $$ 0 $$ | Quotient = $$ 1 $$ |
| $1/2$ | Remainder = $$ 1 $$ | Quotient = $$ 0 $$ |

>So equivalent binary of integral part of decimal is $$ 100 $$.  
Step 2: Conversion of $$ .47 $$ to binary
{: .prompt-tip }

| Calculations | Remainder | Quotient |
|----------|--------|----------|
| $$ 0.47 * 2 $$ | $$ 0.94 $$ | Integral part: $$ 0 $$ |
| $$ 0.94 * 2 $$ | $$ 1.88 $$ | Integral part: $$ 1 $$ |
| $$ 0.88 * 2 $$ | $$ 1.76 $$ | Integral part: $$ 1 $$ |

>So equivalent binary of fractional part of decimal is $$ .011 $$.  
Step 3: Combined the result of step 1 and 2.  
Final answer can be written as:  
$$ 100 + .011 = 100.011 $
{: .prompt-tip }

### 2.1.2 Calculation in binary addition, subtraction

### 2.1.2 One's and Two's Complement methods of binary subtraction

## 2.2 Functions and Boolean Algebra

### 2.2.1 Introduction to Boolean Algebra
